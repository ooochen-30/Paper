### Mixtral of Experts

论文链接：https://arxiv.org/abs/2401.04088

背景：

这篇工作之前的Switch Transformer，GLaM都是闭源的；最顶级的开源模型仍然是稠密模型，MOE模型理论上高效。

解决方案：

1. **基础架构**：它沿用了其前身 **Mistral 7B** 的所有优秀设计，包括分组查询注意力（GQA）和滑动窗口注意力（SWA），这些都是为了提高效率。

   ---

   **分组查询注意力GQA**：将 Q 头分成几组，**组内的所有 Q 头共享同一对 K 和 V 头**。极大地减少了 K 和 V 缓存的大小，从而降低了推理时的显存占用和内存带宽需求，使得模型在处理长序列时更快、更省资源。

   **滑动窗口注意力SWA**：每个 token 只关注它前面一个固定大小的“窗口”内的 token（比如前 4096 个）。将注意力的计算复杂度从 O(n^2) 降低到 O(n*w)，这使得模型可以处理**非常长的上下文**（Mixtral 支持 32k），而不会导致计算量爆炸。

   ---

2. **MoE 改造**：它将 Mistral 7B 中的每一个前馈网络（FFN）层替换成了一个 MoE 层。（其他如Embedding层、Attention层、输出层参数共享，因此虽然是8×7B但**实际总参数量只有47B，激活参数量只有13B**）

3. **8 个专家 (8x)**：每个 MoE 层包含 **8 个专家**。每个专家本身就是一个与 Mistral 7B 中 FFN 层参数量几乎相同的网络（大约 7B）。

4. **Top-2 路由**：和 GLaM 一样，Mixtral 的路由器也为每个 token 选择**最好的 2 个专家**来处理，并将其输出加权求和。