### Direct Preference Optimization: Your Language Model is Secretly a Reward Model

论文链接：https://arxiv.org/abs/2305.18290

**背景**：

主流对齐方法RLHF流程非常复杂且不稳定，主要存在以下问题：

- 流程繁琐：标准的 RLHF 需要三个阶段：
  1. 监督微调 (SFT)。
  2. **训练奖励模型 (Reward Model)**：专门训练一个模型来给回答打分。
  3. **强化学习微调 (RL Fine-tuning)**：使用 PPO 等算法，让语言模型在“获得高分”和“不偏离原始模型”之间找平衡。
- **计算成本高**：训练过程中需要同时维护多个模型（策略模型、参考模型、奖励模型，有时还有价值模型），并且在训练循环中需要不断从语言模型中采样，显存占用大，训练速度慢。
- **超参数敏感且不稳定**：PPO 算法有很多超参数需要调节，训练过程容易崩溃或不收敛，复现难度大。

能不能跳过强化学习，**直接利用偏好数据优化语言模型**？

**解决方案**：

作者从数学上证明了，求解RLHF的目标函数的最优解等价优化于以下的DPO损失函数：
$$
L(\pi_\theta, \pi_{\text{ref}}) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log\sigma \left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right]
$$
DPO 把“强化学习问题”转化为了一个“监督学习问题”。你只需要准备**偏好数据集（Prompt, 好回答, 坏回答）**，然后用标准的梯度下降法直接更新语言模型即可，不需要采样，不需要奖励模型，也不需要复杂的 PPO 算法。

训练过程中，对于偏好回答，相比参考模型，模型会增加生成的概率；对于拒绝回答，相比参考模型，模型会降低生成的概率。



+ 隐式奖励模型：**语言模型本身就可以被视为一个奖励模型**。通过特定的参数化形式，语言模型的输出概率比值直接对应了奖励值的差值。这使得我们可以绕过显式的奖励建模。

+ 工程上极简与稳定

  + 离线数据，**训练时不需要像 PPO 那样在循环中生成文本**，大大降低了计算开销。

  + 省去了**训练和维护额外奖励模型**的步骤。

  + 主要**只需要调节一个 $\beta$ 参数**（控制偏离程度），比 PPO 那一堆超参数好调得多。

    >$\beta$ 通常在0.1到0.5之间，它控制着模型偏离参考模型的程度。
    >
    >+ $\beta$ 趋近于0时，模型会疯狂拟合偏好数据，可能导致过拟合，生成内容多样性下降，甚至出现“奖励黑客”（Reward Hacking，即为了得高分说胡话）
    >+ $\beta$趋近于无穷时，模型完全不敢动，退化为原始的SFT模型。

  + 作为一个标准的分类损失，**训练过程非常稳定**，不容易崩溃。

+ 性能表现优异



DPO虽然去掉了奖励模型，但仍然需要冻结一个**参考模型**（通常是SFT后的模型）来计算概率比值。即DPO**依赖于高质量的SFT模型**。如果SFT模型本身很差，DPO很难将其“救回来”，甚至可能因为约束太强而无法优化。**DPO是“在SFT基础上”的微调**



**数据质量比数量更重要（尤其是“坏”样本）**

DPO的损失函数依赖于偏好数据对，如果坏回答其实并不坏，或者说reject和chosen区别不大，模型会学到错误的信号，导致性能下降。使用**高质量、区分度大**的偏好数据，即使数量较少，效果也优于大量低质量数据



**离线训练 vs 在线迭代**：标准DPO是离线训练的，即使用固定的数据集。RLHF (PPO) 的优势在于可以在线采样，不断收集新数据迭代。标准DPO无法做到这一点。[DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving](https://arxiv.org/abs/2509.17940) 提到了如何通过多轮迭代来模拟在线学习

