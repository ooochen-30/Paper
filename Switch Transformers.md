### Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity

论文链接：https://arxiv.org/abs/2101.03961

背景：

语言模型遵循“模型越大，效果越好”的规律（参数量&计算量），导致**性能提升严重依赖于计算资源的无限增长**。但对于特定任务/输入，或许只需要激活相关的专家区域

将MOE思想应用在大规模Transformer模型所存在的问题：

+ 好的**路由机制**
+ 分布式训练中，一个输入被发送到位于不同计算设备的多个专家时，设备间的**通信开销很大**
+ 由于路由决策的存在，**训练很不稳定**，尤其是bfloat16这样的低精度浮点数时

解决方案：

将Transformer中间的FFN Layer替换成**Switch FFN**

+ 大量的专家+1个轻量级路由器（只选择一个最合适的专家 Top-1）

![1771933797810](C:\Users\86153\Desktop\temp\1771933797810.png)

**专家输出与路由器概率相乘**的目的是让路由器的学习成为可能：路由器的选择是一个“硬”决策（hard decision），argmax操作**不可微分**，损失的梯度无法通过这个决策点回传到路由器，路由器无法学习更新自己的参数。



**负载均衡损失(Load Balancing Loss)**：为了避免总是把token发送给少数几个明星专家，引入负载均衡损失惩罚不均衡的路由行为，鼓励路由器将 token 尽可能均匀地分配给所有专家。

![1771935073109](C:\Users\86153\Desktop\temp\1771935073109.png)



**专家容量 (Expert Capacity)**：在**硬件实现**中，每个专家能处理的 token 数量是有限的。如果某个专家接收到的 token 超过了它的容量，多余的 token 就会被“丢弃”，**直接通过残差连接跳到下一层**，不进行计算。容量的设计是一个权衡，既要避免浪费计算资源，也要减少 token 丢失。（如容量为150，当分配给当前专家200个token发生了路由不均 则前150个token经过这一层为x+f(x)，而后50个则为x）

![1771935235879](C:\Users\86153\Desktop\temp\1771935235879.png)



**低精度训练的稳定性技巧**：只在**路由器内部的计算**中使用更高精度的 float32，而模型的其他部分仍然使用高效的 bfloat16



**更小的参数初始化**：拥有大量专家和硬性路由决策的模型在训练初期非常脆弱，如果参数初始化得太大，经过 softmax 后的**路由器输出可能会变得非常“尖锐”**（例如，某个专家的概率接近 1.0，其他都是 0），训练容易发散或崩溃。论文作者发现，将标准的 Transformer 初始化方案中的**缩放因子 s 减小 10 倍**（例如，从 1.0 降到 0.1），可以极大地提高训练的稳定性和最终的模型质量。这相当于让模型在训练初期对专家的选择更加“温和”和“不确定”，给了负载均衡机制更多的时间来发挥作用，避免了早期的训练崩溃。



**针对稀疏模型的 Dropout 策略 (Expert Dropout)**：Switch Transformer 的参数量比同等计算量的稠密模型大得多（比如几十倍）。在对下游任务进行微调（fine-tuning）时，这些任务的数据量通常很小，巨大的参数量使得模型极易发生**过拟合**。作者选择在微调时，**只在专家内部（即每个 FFN 层）使用一个非常高的 Dropout 率**（比如 0.4）。而在模型其他非专家的部分（如自注意力层、embedding 层），则保持一个较低的、常规的 Dropout 率（比如 0.1）。